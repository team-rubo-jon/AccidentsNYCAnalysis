---
title: "Análisis Exploratorio Datos"
output: html_document
---

## Cargar librerías y lectura de datos:

```{r, message=FALSE}
library(tidyverse)
library(viridis)
library(leaflet)
library(ggplot2)
library(dplyr)
library(GGally)
library(factoextra)
library(cluster)
library(tsibble)
```

```{r}
#data <- read.csv("Vehicle_Collisions.csv")
```

## Lectura datos (temporal)

Jon:

```{r}
#data <- read.csv("/DatosJon/UPNA/3º Ciencia de Datos/S6/Analisis Multivariante y Visualizacion de Datos/Trabajo/Datos/Vehicle_Collisions.csv")
```

Rubén:

```{r}
#data <- read.csv("C:/Users/ruben/OneDrive/Escritorio/UPNA/CIENCIA DE DATOS/6ºSemestre/AnalisisMultivarianteVisualizacionDatos/Trabajo Final/Motor_Vehicle_Collisions_-_Crashes.csv")
```

Fermin:

```{r}
data <- read.csv("C:/Users/mintx/Mi unidad/CdD UPNA/.3.2/Analisis multivariante y visualizacion de datos/Trabajo/Vehicle_Collisions.csv")
```

# INTRODUCCION

En nuestro conjunto de datos, se recoje información sobre accidentes de tráfico en la ciudad de Nueva York. La información proviene de la página de datos del gobierno de EEUU, específicamente del apartado correspondiente a la ciudad de Nueva York.

Cada individuo corresponde a un accidente de tráfico, que se representa como una fila en el conjunto de datos. Las columnas incluyen información como la fecha y hora del accidente, la ubicación, el tipo de vehículo, y los factores contribuyentes al accidente entre otros muchos. Los accidentes se han almacenado desde el 1/01/2013 hasta la actualidad.

Son accidentes en los que se ha rellenado un informe policial, por lo que podrían no incluirse algunos accidentes menores o accidentes que no han sido reportados a la policía. Ya que este informe es necesario para los accidentes donde hay fallecidos, o daños de al menos 1000\$.

Además, como la forma de recoger los datos ha ido cambiando a lo largo de los años, es posible que hace años no se almacenase información como puede ser las coordenadas, y que en los últimos años con la implementación de dispositivos electrónicos se almacenen las coordenadas exactas de cada accidente. También habrá diferencias en la información dependiendo de la forma en la que el agente de policia correspondiente haya guardado los datos, ya que no son sensores sin fallo los que determinan la mayoría de variables (como pueden ser el tipo de vehículo o factor contribuyentes de la colisión).

Con todo esto, en nuestro trabajo trataremos de analizar las opciones que nos da el conjunto, e ir mostrando los resultados obtenidos visualmente. Y así poder encontrar patrones, agrupar los datos, o ver si podemos encontrar relación entre las variables que nos permitan entender mejor el problema tratado.

# LIMPIEZA INICIAL Y MUESTREO

En primer lugar, observamos que el dataset contiene más de 2 millones de filas, pero que además muchas de las variables están prácticamente vacías o con información redundante. Además, dada la naturaleza de los datos geolocalizados, hay ejemplos que no contienen esa información y no nos son de ayuda.

Por tanto, vamos a realizar una primera limpieza del dataset, eliminando tanto variables redundantes o con poca información, como ejemplos que no nos aporten la información suficiente.

## Eliminar variables:

-   *ZIP.CODE* (ya tenemos la variable *BOROUGH* que contiene el barrio)
-   *ON.STREET.NAME*, *OFF.STREET.NAME*, *CROSS.STREET.NAME* (es información que ya se almacena en las coordenadas)
-   *VEHICLE.TYPE.CODE.3*, *VEHICLE.TYPE.CODE.4*, *VEHICLE.TYPE.CODE.5* (prácticamente todos son valores nulos)
-   *CONTRIBUTING.FACTOR.VEHICLE.2*, *CONTRIBUTING.FACTOR.VEHICLE.3*, *CONTRIBUTING.FACTOR.VEHICLE.4*, *CONTRIBUTING.FACTOR.VEHICLE.5* (prácticamente todos son valores nulos)
-   *COLLISION_ID* (es un identificador que no aporta información)
-   *LOCATION* (es la combinación exacta de las variables *LATITUDE* Y *LONGITUDE*)

```{r}
data_cleaned <- data |> 
  dplyr::select(-ZIP.CODE, -ON.STREET.NAME, -OFF.STREET.NAME, -CROSS.STREET.NAME, -VEHICLE.TYPE.CODE.3, -VEHICLE.TYPE.CODE.4, -VEHICLE.TYPE.CODE.5, -COLLISION_ID, -CONTRIBUTING.FACTOR.VEHICLE.5, -CONTRIBUTING.FACTOR.VEHICLE.4, -CONTRIBUTING.FACTOR.VEHICLE.3, -CONTRIBUTING.FACTOR.VEHICLE.2,-LOCATION)
```

Con el filtrado realizado, obtenemos un dataset mucho más manejable en cuanto a variables se refiere, que nos permitirá trabajar mejor el problema. Haciendo un pequeño resumen de las variables que nos quedan:

-   CRASH.DATE: Fecha en la que se produció el accidente (Date)
-   CRASH.TIME: Hora en la que se produció el accidente (Time)
-   BOROUGH: Barrio de Nueva York donde sucedió el accidente (String)
-   LATITUDE/LONGITUDE: Coordenadas geográficas (Int)
-   NUMBER.OF.(PERSONS/PEDESTRIANS/CYCLIST/MOTORIST).(KILLED/INJURIED): Número y tipo de personas implicadas en el accidente, diferenciando heridos y fallecidos (Int)
-   CONTRIBUTING.FACTOR.VEHICLE: Causa principal del accidente (String)
-   VEHICLE.TYPE.CODE.1: Tipo del primero vehiculo implicado (String)
-   VEHICLE.TYPE.CODE.2: Tipo del segundo vehiculo implicado (String)

## Eliminar individuos:

-   Individuos que no tengan *LATITUDE* y/o *LONGITUDE* (No nos sirven para localizar el accidente)
-   Individuos que no tengan ningún Vehiculo implicado en *VEHICLE.TYPE.CODE*
-   Individuos que no tengan barrio *BOROUGH* (ya que tenemos los suficientes que si tienen esa información)
-   Individuos que no tengan una razon que haya causado el accidente *CONTRIBUTING.FACTOR.VEHICLE*
-   Individuos con valor de localización erronea (Coordenadas 0) *LONGITUDE*

```{r}
data_cleaned <- data_cleaned |> 
  filter(
    !is.na(LATITUDE) & !is.na(LONGITUDE),
    !(is.na(VEHICLE.TYPE.CODE.1) & is.na(VEHICLE.TYPE.CODE.2)),
    !(VEHICLE.TYPE.CODE.1 == "" & VEHICLE.TYPE.CODE.2 == ""),
    !(BOROUGH == ""),
    !(CONTRIBUTING.FACTOR.VEHICLE.1 == "Unspecified"),
    !(CONTRIBUTING.FACTOR.VEHICLE.1 == ""),
    !(LATITUDE==0),
    !(LONGITUDE==0))
```

## Renombrar variables:

Antes de empezar a trabajar con los datos, vamos a renombrar ciertas variables para facilitar la programación.

```{r}
data_cleaned <- data_cleaned |> 
  rename(
    DATE = CRASH.DATE,
    TIME = CRASH.TIME,
    NUM_PERSONS_INJURED = NUMBER.OF.PERSONS.INJURED,
    NUM_PERSONS_KILLED = NUMBER.OF.PERSONS.KILLED,
    NUM_PEDESTRIANS_INJURED = NUMBER.OF.PEDESTRIANS.INJURED,
    NUM_PEDESTRIANS_KILLED = NUMBER.OF.PEDESTRIANS.KILLED,
    NUM_CYCLIST_INJURED = NUMBER.OF.CYCLIST.INJURED,
    NUM_CYCLIST_KILLED = NUMBER.OF.CYCLIST.KILLED,
    NUM_MOTORIST_INJURED = NUMBER.OF.MOTORIST.INJURED,
    NUM_MOTORIST_KILLED = NUMBER.OF.MOTORIST.KILLED,
    CAUSE = CONTRIBUTING.FACTOR.VEHICLE.1,
    VEHICLE_1 = VEHICLE.TYPE.CODE.1,
    VEHICLE_2 = VEHICLE.TYPE.CODE.2
  )
  
```

## Eliminación de filas incompletas:

### Personas afectadas

Vamos a comprobar si el número de personas heridas o fallecidas coincide con el número de heridos o fallecidos que se han dado en la fila. Para ello, vamos a crear una tabla con los diferentes tipos de heridos y fallecidos, y luego vamos a comprobar si el número total coincide con el número de personas heridas o fallecidas que se han dado en la fila. 

```{r}
type_injured <- data_cleaned |> 
  dplyr::select(NUM_PERSONS_INJURED, NUM_PEDESTRIANS_INJURED, NUM_CYCLIST_INJURED, NUM_MOTORIST_INJURED,
         NUM_PERSONS_KILLED, NUM_PEDESTRIANS_KILLED, NUM_CYCLIST_KILLED, NUM_MOTORIST_KILLED) |> 
  pivot_longer(cols = everything(), names_to = 'TYPE_OF_INJURED', values_to = 'NUMBER')

# quiero saber el total de de cada tipo

type_injured |> 
  group_by(TYPE_OF_INJURED) |> 
  summarise(NUMBER = sum(NUMBER)) |> 
  arrange(desc(NUMBER))

# ahora mostramos los indices de los que no coinciden
filas_erroneas <- data_cleaned |> 
  dplyr::select(NUM_PERSONS_INJURED, NUM_PEDESTRIANS_INJURED, NUM_CYCLIST_INJURED, NUM_MOTORIST_INJURED,
         NUM_PERSONS_KILLED, NUM_PEDESTRIANS_KILLED, NUM_CYCLIST_KILLED, NUM_MOTORIST_KILLED) |> 
  mutate(ROW_ID = row_number(),
         TOTAL_NUMBER_INJURED = rowSums(across(c(NUM_PEDESTRIANS_INJURED, NUM_CYCLIST_INJURED, NUM_MOTORIST_INJURED))),
         TOTAL_NUMBER_KILLED = rowSums(across(c(NUM_PEDESTRIANS_KILLED, NUM_CYCLIST_KILLED, NUM_MOTORIST_KILLED)))) |>
  dplyr::select(ROW_ID, NUM_PERSONS_INJURED, TOTAL_NUMBER_INJURED, NUM_PERSONS_KILLED, TOTAL_NUMBER_KILLED) |> 
  filter(NUM_PERSONS_INJURED != TOTAL_NUMBER_INJURED | NUM_PERSONS_KILLED != TOTAL_NUMBER_KILLED)

filas_erroneas
```

Podemos ver como existen filas que no cumplen, por ello eliminamos las filas erroneas, aquellas filas cuyo numero de personas heridas o fallecidas no coincide con el total de heridos o fallecidos que se han dado en la fila.


```{r}
indices_erroneos <- filas_erroneas |> 
  pull(ROW_ID)

data_cleaned <- data_cleaned[-indices_erroneos, ]

```

## Eliminacion de valores incorrectos de la variable 'CAUSE'

```{r}
data_cleaned <- data_cleaned |> 
  mutate(CAUSE = recode(CAUSE,
    "Cell Phone (hand-Held)" = "Cell Phone (hand-held)",
    "Drugs (Illegal)" = "Drugs (illegal)",
    "Illnes" = "Illness",
    "Other Electronic Device" = "Other Devices",
    "Drugs (illegal)" = "Drugs (illegal)",
    "Unspecified" = "Unspecified",
    "Pedestrian/Bicyclist/Other Pedestrian Error/Confusion" = "Pedestrian Error/Confusion",
    "Aggressive Driving/Road Rage" = "Aggressive Driving"
  )) |> 
  filter(CAUSE != 1, CAUSE != 80) 
```


## Muestreo:

Una vez eliminados los individuos que no son de interes, el dataset se reduce a 893275 filas. Es más manejable pero seguimos teniendo demasiadas observaciones, por lo que para trabajar con los datos de forma más comoda hemos decidido hacer un muestreo de 20.000 observaciones


```{r}
set.seed(123)
data_sampled <- sample_n(data_cleaned, 20000)
```

Como a partir de ahora trabajaremos con esta muestra en lugar del dataset completo, por comodidad lo almacenaremos en un nuevo archivo para solo tener que leer este.

```{r}
write.csv(data_sampled, "data_sampled.csv", row.names = FALSE)
```

### Leer muestra:

```{r}
data_sampled <- read.csv("data_sampled.csv")
```

# ANALISIS DESCRIPTIVO

Una vez obtenido el dataset con el que trabajaremos, vamos a visualizar las diferentes variables y sus comportamientos para tratar de entender mejor con que datos estamos trabajando antes de aplicar las diferentes técnicas.

Vamos a mostrar un histograma por cada variable o variables que creamos que van conjuntamente, y así ver la distribución de los datos. Además, también mostraremos la matriz de correlación entre las variables numéricas, para ver si hay alguna relación entre ellas.

## Variables 'DATE' y 'TIME'

```{r}
data_time <- data_sampled |> 
  mutate(DATE_DAY = as.Date(DATE, format = "%m/%d/%Y")) |>  
  mutate(DATE_MONTH = yearmonth(DATE_DAY),
         DATE_YEAR = year(DATE_DAY)) |> 
  add_count(DATE_DAY, name = "FREQ_DAY") |> 
  add_count(DATE_MONTH, name = "FREQ_MONTH") |> 
  add_count(DATE_YEAR, name = "FREQ_YEAR") |>
  dplyr::select(DATE_DAY, DATE_MONTH, DATE_YEAR, FREQ_DAY, FREQ_MONTH, FREQ_YEAR) |> 
  arrange(DATE_DAY)


data_time |>
  ggplot(aes(x = DATE_DAY, y = FREQ_DAY)) + 
  geom_line(color = "steelblue") + 
  labs(title = "Frecuencia de Accidentes por Fecha",
       x = "Fecha",
       y = "Frecuencia") +
  theme_minimal()

data_time |>
  ggplot(aes(x = DATE_MONTH, y = FREQ_MONTH)) + 
  geom_line(color = "steelblue") + 
  labs(title = "Frecuencia de Accidentes por Mes",
       x = "Mes",
       y = "Frecuencia") +
  theme_minimal()

data_time |>
  ggplot(aes(x = DATE_YEAR, y = FREQ_YEAR)) + 
  geom_line(color = "steelblue") + 
  labs(title = "Frecuencia de Accidentes por Año",
       x = "Año",
       y = "Frecuencia") +
  theme_minimal()

```
```{r}
# ahora quiero agrupar por meses, de enero a diciembre, cuantos accidentes hay
data_time |> 
  group_by(DATE_MONTH) |> 
  summarise(FREQ_MONTH = sum(FREQ_MONTH)) |> 
  mutate(MONTH = month(DATE_MONTH, label = TRUE)) |> 
  ggplot(aes(x = MONTH, y = FREQ_MONTH)) + 
  geom_bar(stat = "identity", fill = "steelblue") + 
  labs(title = "Frecuencia de Accidentes por Mes",
       x = "Mes",
       y = "Frecuencia") +
  theme_minimal()

# ahora por dia
data_time |> 
  ggplot(aes(x = DATE_YEAR, y = FREQ_YEAR)) + 
  geom_bar(stat = "identity", fill = "steelblue") + 
  labs(title = "Frecuencia de Accidentes por Año",
       x = "Mes",
       y = "Frecuencia") +
  theme_minimal()
```
Se puede ver claramente como en el año del covid la frecuencia de accidentes disminuye significativamente, debido a que la gente no se movía tanto como en años anteriores. Sin embargo, la frecuencia de accidentes se mantiene casi constante, incluso llegando a disminuir en en 2022. Esto tambien se puede deber a que a raiz del covid la gente ha cambiado su forma de moverse, y ahora se mueven más en bicicleta o andando, lo que puede hacer que haya más accidentes de este tipo. También, hemos comprobado que después del covid, en EEUU, modificaron leyes de tráfico que hizo que la gente sea mas consciente de la seguridad vial, y por lo tanto, haya menos accidentes.


## Variable 'BOROUGH'

```{r}
# Mostramos el histograma de los barrios, además del box plot:
data_sampled |> 
  ggplot(aes(x = BOROUGH)) +
  geom_histogram(stat = "count", fill = "steelblue") +
  labs(title = "Distribución de Accidentes por Barrio",
       x = "Barrio",
       y = "Número de Accidentes")
```

## Variables 'LATITUDE' y 'LONGITUDE

Para el caso de latitude y longitude, vamos a mostrar un mapa con la localización de los accidentes en el mapa de Nueva York. Para ello, vamos a usar la librería leaflet, que nos permite mostrar mapas interactivos.

```{r, results='asis', echo=FALSE}
data_sampled |> 
  leaflet() |> 
  addTiles() |> 
  addCircleMarkers(
    lng = ~LONGITUDE,
    lat = ~LATITUDE,
    radius = 2,
    color = "red",
    stroke = FALSE,
    fillOpacity = 0.6,
    popup = ~paste("Lat:", LATITUDE, "<br>Lon:", LONGITUDE)
  )
```

## Variables Personas Afectadas

Ahora, vamos a mostrar un histograma comparando la cantidad de personas heridas frente a la cantidad de personas muertas:

```{r}
# 1. Crear un resumen total
datos_resumen <- data_sampled |> 
  summarise(
    Total_Heridos = sum(NUM_PERSONS_INJURED, na.rm = TRUE),
    Total_Muertos = sum(NUM_PERSONS_KILLED, na.rm = TRUE)
  )

# 2. Reestructurar para graficar
datos_long <- tidyr::pivot_longer(
  datos_resumen,
  cols = everything(),
  names_to = "Tipo",
  values_to = "Personas"
)

# 3. Crear el histograma comparativo
ggplot(datos_long, aes(x = Tipo, y = Personas, fill = Tipo)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = Personas), vjust = -0.5, size = 3, fontface = "bold") +
  labs(title = "Total de personas heridas vs muertas",
       x = "", y = "Número de personas") +
  scale_fill_manual(values = c("firebrick", "darkblue")) +
  theme_minimal()
```

Ahora, podemos comprobar el número de viandantes, ciclistas y motoristas que han resultado heridos o muertos en los accidentes:

```{r, fig.width=15} 
# 1. Crear un resumen total
datos_resumen <- data_sampled |> 
  summarise(
    Total_Heridos = sum(NUM_PERSONS_INJURED, na.rm = TRUE),
    Heridos_Pedestrians = sum(NUM_PEDESTRIANS_INJURED, na.rm = TRUE),
    Heridos_Cyclists = sum(NUM_CYCLIST_INJURED, na.rm = TRUE),
    Heridos_Motorists = sum(NUM_MOTORIST_INJURED, na.rm = TRUE),
    Total_Muertos = sum(NUM_PERSONS_KILLED, na.rm = TRUE),
    Muertos_Pedestrians = sum(NUM_PEDESTRIANS_KILLED, na.rm = TRUE),
    Muertos_Cyclists = sum(NUM_CYCLIST_KILLED, na.rm = TRUE),
    Muertos_Motorists = sum(NUM_MOTORIST_KILLED, na.rm = TRUE)
  )

# 2. Pasar a formato largo
datos_long <- datos_resumen |> 
  pivot_longer(cols = everything(),
               names_to = c("Estado", "Tipo"),
               names_sep = "_",
               values_to = "Total")

# 3. Gráfico separado por estado (herido/muerto)
datos_long |>  
  filter(Estado != 'Total') |> 
ggplot(aes(x = Tipo, y = Total, fill = Tipo)) +
  geom_col(show.legend = TRUE) +  # Activamos la leyenda
  geom_text(aes(label = Total), vjust = -0.5, size = 4) +
  facet_wrap(~Estado, scales = "free_y") +
  labs(
    title = "Total de personas heridas y muertas por tipo",
    x = NULL,  # Quitamos la etiqueta del eje x
    y = "Total",
    fill = "Tipo de persona"  # Título de la leyenda
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),       # Oculta los nombres debajo de las barras
    axis.ticks.x = element_blank(),      # Oculta las marcas del eje x
    legend.position = "bottom"           # Posiciona la leyenda debajo del gráfico
  )
```

*CON LOS TIPOS DE VEHÍCULOS NO SÉ MUY BIEN QUE HACER*

Veamos ahora la matriz de correlación:

```{r, fig.width=15, message=FALSE}
ggpairs(data_sampled, columns = c("NUM_PERSONS_INJURED", "NUM_PEDESTRIANS_INJURED", "NUM_CYCLIST_INJURED", "NUM_MOTORIST_INJURED", "NUM_PERSONS_KILLED", "NUM_PEDESTRIANS_KILLED", "NUM_CYCLIST_KILLED", "NUM_MOTORIST_KILLED"), 
        title = "Matriz de Correlación entre Variables")
```
*No se yo si la correlación tiene mucho sentigo hacerlo o si es significativa...*


Vamos a mostrar un histograma por cada variable o variables que creamos que van conjuntamente, y así ver la distribución de los datos. Además, también mostraremos la matriz de correlación entre las variables numéricas, para ver si hay alguna relación entre ellas.


## Variable 'CAUSE'

Primero, vamos a ver la frecuencia de los diferentes factores contribuyentes al accidente. Para ello, vamos a crear una tabla con los diferentes factores contribuyentes y su frecuencia.


```{r}
causes_cleaned <- data_sampled |> 
  count(CAUSE, name = "FREQUENCY") |>  
  arrange(desc(FREQUENCY))

```

Ahora vamos a graficar todos los factores contribuyentes, y su frecuencia. Para ello, vamos a usar un gráfico de barras.

```{r}
causes_cleaned |> 
  ggplot(aes(x = reorder(CAUSE, FREQUENCY), y = FREQUENCY)) + 
  geom_bar(stat = "identity", fill = "steelblue") + 
  # coord_flip() +  # Rota el gráfico para mejor visualización
  labs(title = "Total Frecuencia de Factores Contribuyentes",
       x = "Factor Contribuyente",
       y = "Frecuencia") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle=60, hjust=1,size=6),
        axis.title = element_text(size=10),
        title = element_text(size=12),
        axis.title.x=element_blank(),
        legend.title = element_blank()
        )
```

```{r}
data_sampled |> 
  ggplot(aes(x = CAUSE, y = NUM_PERSONS_INJURED)) +
  geom_boxplot(fill = "steelblue") +
  labs(title = "Personas heridas según la causa del accidente",
       x = "Causa (agrupadas las menos frecuentes)",
       y = "Número de heridos") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


(Aqui añadir histogramas, matrices de correlacion de las variables, eliminar los individuos localizados muy lejos, distribuciones de cada variable (violines)...)



# METODOLOGÍA MULTIVARIANTE

## PCA

Vamos a realizar un análisis de componentes principales (PCA) para reducir la dimensionalidad de los datos y ver si podemos encontrar patrones en los datos. Sin embargo, lo primero que vamos hacer es comprobar si tiene sentido hacer un pca con los datos numéricos que tenemos. Aunque en la matriz de correlación ya hemos visto que no hay correlaciones significativas en los datos, vamos a comprobar mediante estadísticos si los datos son adecuados para realizar un PCA.

Realizar el test de KMO y la prueba de esfericidad de Bartlett para comprobar si los datos son adecuados para realizar un PCA:

1. Test de KMO: El test de KMO mide la adecuación de los datos para realizar un PCA. Un valor de KMO mayor a 0.6 indica que los datos son adecuados para realizar un PCA.

```{r}
library(psych)

datos_numeric <- data_sampled |> 
  select(NUM_PERSONS_INJURED, NUM_PEDESTRIANS_INJURED, NUM_CYCLIST_INJURED, NUM_MOTORIST_INJURED,
         NUM_PERSONS_KILLED, NUM_PEDESTRIANS_KILLED, NUM_CYCLIST_KILLED, NUM_MOTORIST_KILLED)

KMO(datos_numeric)

```
Observamos que todas las variables tienen un KMO menor a 0.6, por lo que no es interesante realizar un PCA.

2. Test de esfericidad de Bartlett: 

```{r}
cortest.bartlett(cor(datos_numeric), n = nrow(datos_numeric))
```
El test de esfericidad de Bartlett acepta (p-valor<0.05) la hipótesis nula de que la matriz de correlación es una matriz identidad. Por lo tanto, los datos se encuentran correlados.

En conclusión, pese a que los datos, según el test de esfericidad de Bartlett se encuentran correlados, no es interesante realizar un PCA, ya que el KMO no es adecuado. Sin embargo, igual es interesante analizar la posibilidad de realizar PCA agrupando las variables por heridos y muertos.

### PCA con variables agrupadas:

#### PCA con heridos

```{r}
# Seleccionar solo las variables de personas heridas
datos_heridos <- data_sampled |> 
  select(NUM_PERSONS_INJURED, NUM_PEDESTRIANS_INJURED, 
         NUM_CYCLIST_INJURED, NUM_MOTORIST_INJURED)

# Escalar (normalizar) los datos
datos_heridos_scaled <- scale(datos_heridos)

```

Igual que antes, analizamos la posibilidad de realizar un PCA con los datos agrupados. Para ello, vamos a realizar el test de KMO y la prueba de esfericidad de Bartlett.

```{r}
# Calcular KMO
KMO(datos_heridos_scaled)

# Test de Bartlett
cortest.bartlett(cor(datos_heridos_scaled), n = nrow(datos_heridos_scaled))

```

De manera similar al caso general, obtenemos que no es interesante realizar un PCA, pese a que los datos se encuentran relacionados (p-valor<0.05 en el test de Bartlett) el KMO vuelve a rechazar la idea de realizar el PCA, obtenemos un valor muy por debajo de 0.6.

#### PCA con muertos

```{r}
# Seleccionar solo las variables de personas muertas
datos_muertos <- data_sampled |> 
  select(NUM_PERSONS_KILLED, NUM_PEDESTRIANS_KILLED, 
         NUM_CYCLIST_KILLED, NUM_MOTORIST_KILLED)

# Escalar (normalizar) los datos
datos_muertos_scaled <- scale(datos_muertos)
```

Al igual que antes, analizamos la posibilidad de realizar un PCA con los datos agrupados. Para ello, vamos a realizar el test de KMO y la prueba de esfericidad de Bartlett.

```{r}
# Calcular KMO
KMO(datos_muertos_scaled)

# Test de Bartlett
cortest.bartlett(cor(datos_muertos_scaled), n = nrow(datos_muertos_scaled))
```
De nuevo, obtenemos la misma conclusión que en los dos casos anteriores. Por lo tanto, no es interesante realizar un PCA con los datos agrupados.

## Componentes principales

## Clusterizaciones

### 1. Seleccionar variables numéricas relevantes:

```{r}
data_cluster <- data_sampled |> 
  select(NUM_PERSONS_INJURED, NUM_PEDESTRIANS_INJURED, NUM_CYCLIST_INJURED, NUM_MOTORIST_INJURED,
         NUM_PERSONS_KILLED, NUM_PEDESTRIANS_KILLED, NUM_CYCLIST_KILLED, NUM_MOTORIST_KILLED)

# Escalar (normalizar) los datos
data_scaled <- scale(data_cluster)
```

### 2. Determinar el número óptimo de clusters:

```{r}
wss <- vector()
set.seed(123)
for (k in 1:10) {
  km <- kmeans(data_scaled, centers = k, nstart = 5)  # menos repeticiones
  wss[k] <- km$tot.withinss
}

plot(1:10, wss, type = "b", pch = 19,
     xlab = "Número de clusters",
     ylab = "Suma de cuadrados intra-clúster (WSS)",
     main = "Método del codo (optimizado)")

```

### 3. Realizar el clustering y visualizar:

```{r}
set.seed(123)
kmeans_result <- kmeans(data_scaled, centers = 4, nstart = 25)

# Visualizar clústeres en el espacio PCA
fviz_cluster(kmeans_result, data = data_scaled,
             ellipse.type = "norm",
             geom = "point",
             palette = "jco",
             ggtheme = theme_minimal())

```
Vemos que no se obtiene nada claro. Vamos a probar con otro tipo de clustering, el clustering jerárquico. 

### Clustering jerarquico

```{r}
# Calcular la matriz de distancias
dist_matrix <- dist(data_scaled, method = "euclidean")

# Realizar el clustering jerárquico
hc <- hclust(dist_matrix, method = "complete") # --> Mirar a ver que método es mejor
```
```{r}
# Visualizar el dendrograma
plot(hc, labels = FALSE, hang = -1, main = "Dendrograma de Clustering Jerárquico")

# Cortamos:
rect.hclust(hc, k = 3, border = "red")
clusters <- cutree(hc, k = 3)
table(clusters)
```

### Agrupar los datos por barrios y clusterizar:

```{r}
data_borough_sum <- data_sampled |> 
  group_by(BOROUGH) |> 
  summarise(
    total_injured = sum(NUM_PERSONS_INJURED, na.rm = TRUE),
    total_killed = sum(NUM_PERSONS_KILLED, na.rm = TRUE),
    total_ped_injured = sum(NUM_PEDESTRIANS_INJURED, na.rm = TRUE),
    total_cyclist_injured = sum(NUM_CYCLIST_INJURED, na.rm = TRUE),
    total_motorist_injured = sum(NUM_MOTORIST_INJURED, na.rm = TRUE),
    total_ped_killed = sum(NUM_PEDESTRIANS_KILLED, na.rm = TRUE),
    total_cyclist_killed = sum(NUM_CYCLIST_KILLED, na.rm = TRUE),
    total_motorist_killed = sum(NUM_MOTORIST_KILLED, na.rm = TRUE)
  ) |> 
  na.omit()

```

Escalar los datos:

```{r}
data_scaled <- data_borough_sum |> 
  select(-BOROUGH) |> 
  scale()
```

Clustering jerárquico:

```{r}
dist_matrix <- dist(data_scaled)
hc <- hclust(dist_matrix, method = "complete")

# Dendrograma
plot(hc, labels = data_borough_sum$BOROUGH, main = "Dendrograma de barrios (suma total)")
rect.hclust(hc, k = 4, border = "blue")  # Prueba con k = 2, 3, 4, etc.
```
Añadir un gráfico de barras comparando las variables entre los clústeres.

Hacer un mapa donde se coloree cada barrio según el clúster.

## Análisis discriminante

## Análisis de correspondencia

### Variable 'CAUSE'

#### Prueba 1

Vamos a realizar una clusterización de los diferentes factores contribuyentes al accidente. Para ello, vamos a usar el algoritmo K-means, que nos permite agrupar los diferentes factores en diferentes grupos.

```{r}
cause_norm <- causes_cleaned |>
  mutate(FREQUENCY_NORM = scale(FREQUENCY)) |>
  dplyr::select(CAUSE, FREQUENCY_NORM)

rownames(cause_norm) <- cause_norm$CAUSE
cause_norm <- cause_norm |> 
  dplyr::select(-CAUSE)

fviz_nbclust(x = cause_norm, 
             FUNcluster = kmeans, 
             method = "wss", 
             k.max = 15, 
             nstart = 50) +
  labs(title = "Número óptimo de clusters (WSS)")

```



```{r, warning=FALSE, message=FALSE}

pam_clusters <- pam(x = cause_norm, k = 4, metric = "manhattan")

# Visualizar con fviz_cluster usando dos dimensiones
# fviz_cluster(pam_clusters, 
#              data = cause_norm,
#              show.clust.cent = TRUE, 
#              ellipse.type = "euclid",
#              star.plot = TRUE, 
#              repel = TRUE) +
#   labs(title = "Resultados clustering K-means (CAUSE)") +
#   theme_bw() + 
#   theme(legend.position = "none")


resultados <- data.frame(ciudad = names(pam_clusters$cluster), cluster = as.factor(pam_clusters$cluster)) |> 
    arrange(cluster)

library(igraph)
library(tidygraph)
library(ggraph)

datos_graph <- graph_from_data_frame(d = resultados, directed = TRUE)
datos_graph <- as_tbl_graph(datos_graph)

# Se añade información sobre a que cluster pertenece cada observacion
datos_graph <- datos_graph |> 
    activate(nodes) |> 
    left_join(resultados, by = c(name = "ciudad"))

ggraph(graph = datos_graph) + 
  geom_edge_link(alpha = 0.5) + 
  geom_node_point(aes(color = cluster)) +
  geom_node_text(aes(label = name), repel = TRUE, alpha = 0.5, size = 3) + 
  labs(title = "Resultados clustering K-means") +
  theme_graph()

```

#### Prueba 2

Rpealizar PCA con otro tio de metricas.

1. Crear una matriz de características para cada causa

```{r}

# Ejemplo: sumar heridos y muertos promedio por causa
cause_features <- data_sampled |> 
  group_by(CAUSE) |> 
  summarise(
    N = n(),
    avg_injured = mean(NUM_PERSONS_INJURED, na.rm = TRUE),
    avg_killed = mean(NUM_PERSONS_KILLED, na.rm = TRUE),
    # avg_cyclist_injured = mean(NUM_CYCLIST_INJURED, na.rm = TRUE),
    # avg_motorist_injured = mean(NUM_MOTORIST_INJURED, na.rm = TRUE),
    # avg_pedestrian_injured = mean(NUM_PEDESTRIANS_INJURED, na.rm = TRUE)
  ) |> 
  ungroup()

```

2. Normalizar los datos y aplicar PCA

```{r}
# Normalizar
cause_features_scaled <- cause_features |> 
  column_to_rownames("CAUSE") |> 
  scale()

# PCA
pca_res <- prcomp(cause_features_scaled, center = TRUE, scale. = TRUE)
biplot(pca_res, main = "PCA de las causas")

library(ggbiplot)
ggbiplot(pca_res, choices = c(1,2))
```
3. Clustering

```{r}
# Determinar número de clusters óptimo
fviz_nbclust(cause_features_scaled, kmeans, method = "wss")

# K-means clustering
set.seed(123)
km <- kmeans(cause_features_scaled, centers = 7, nstart = 50)

# Visualizar clusters
fviz_cluster(km, data = cause_features_scaled, 
             show.clust.cent = TRUE, 
             repel = TRUE) +
  theme_minimal() +
  labs(title = "Clustering de causas de accidente")
```
#### Prueba 3

Establecer similitudes entre los nombres de las causas.

```{r}
library(stringdist)

# Crear matriz de distancias entre nombres de causas
dist_matrix <- stringdistmatrix(causes_cleaned$CAUSE, causes_cleaned$CAUSE, method = "jw")  # método Jaro-Winkler, o "lv" para Levenshtein
rownames(dist_matrix) <- causes_cleaned$CAUSE
colnames(dist_matrix) <- causes_cleaned$CAUSE
```

Clustering jerarquico

```{r, warning=FALSE}
hc <- hclust(as.dist(dist_matrix), method = "ward.D2")
plot(hc, main = "Dendograma de Causas", cex = 0.7)


fviz_dend(x = hc, cex = 0.5, main = "Dendrograma - ward", xlab = "observaciones",
    ylab = "distancia", sub = "", horiz = TRUE)

```

Cortar endograma

```{r, warning=FALSE}
fviz_dend(x = hc,
          k = 8,
          k_colors = c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd", "#8c564b",  "#e377c2", "#7f7f7f"),
          color_labels_by_k = TRUE,
          rect = TRUE,
          #rect_border = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
          rect_fill = TRUE,
          cex = 0.5,
          main = "Dendrograma - ward",
          xlab = "observaciones",
          ylab = "distancia",
          sub = "")

fviz_dend(x = hc, k = 8, k_colors = c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd", "#8c564b",  "#e377c2", "#7f7f7f"),
    color_labels_by_k = TRUE, cex = 0.5, type = "circular")

```

Ahora vamos a agrupar las causas en los 8 grupos

```{r}
# Asignar cada causa a uno de los 8 clusters
grupo_causas <- cutree(hc, k = 8)

# Crear data frame con causa original y grupo asignado
causas_grupo <- data.frame(
  CAUSE = names(grupo_causas),
  GRUPO_CAUSA = grupo_causas
)

# unimos al dataframe original
data_sampled <- data_sampled |> 
  left_join(causas_grupo, by = "CAUSE")


```


## Analisis discriminante 

```{r}
library(MASS)

data_sampled$BOROUGH_NUM <- as.numeric(factor(data_sampled$BOROUGH))

modelo_lda <- lda(formula = GRUPO_CAUSA ~ NUM_PERSONS_INJURED + NUM_PERSONS_KILLED + BOROUGH_NUM, data = data_sampled)

new_data <- data_sampled[, c("NUM_PERSONS_INJURED", "NUM_PERSONS_KILLED", "BOROUGH_NUM")]

predicciones <- predict(object = modelo_lda, 
                        newdata = data_sampled[, c("NUM_PERSONS_INJURED", "NUM_PERSONS_KILLED", "NUM_PEDESTRIANS_INJURED", "NUM_PEDESTRIANS_KILLED", "NUM_CYCLIST_INJURED", "NUM_CYCLIST_KILLED", "NUM_MOTORIST_INJURED", "NUM_MOTORIST_KILLED", "BOROUGH_NUM")], 
                        method = "predictive")

table(data_sampled$GRUPO_CAUSA, predicciones$class, dnn = c("Clase real", "Clase predicha"))

```
Igual no tiene mucho sentido, predice casi siempre el 1.


# CONCLUSIONES


































# Pruebas

## Contributing Factor Vehicle

```{r}
contr_v1 <- data |> 
  dplyr::select(CONTRIBUTING.FACTOR.VEHICLE.1) |> 
  mutate(CONTR_V1 = CONTRIBUTING.FACTOR.VEHICLE.1) |> 
  filter(if_any(everything(), ~ . != "")) |> 
  count(CONTR_V1, name='FREQUENCY')

contr_v2 <- data |> 
  dplyr::select(CONTRIBUTING.FACTOR.VEHICLE.2) |> 
  mutate(CONTR_V2 = CONTRIBUTING.FACTOR.VEHICLE.2) |> 
  filter(if_any(everything(), ~ . != "")) |> 
  count(CONTR_V2, name='FREQUENCY')

contr_v3 <- data |> 
  dplyr::select(CONTRIBUTING.FACTOR.VEHICLE.3) |> 
  mutate(CONTR_V3 = CONTRIBUTING.FACTOR.VEHICLE.3) |> 
  filter(if_any(everything(), ~ . != "")) |> 
  count(CONTR_V3, name='FREQUENCY')

contr_v4 <- data |> 
  dplyr::select(CONTRIBUTING.FACTOR.VEHICLE.4) |> 
  mutate(CONTR_V4 = CONTRIBUTING.FACTOR.VEHICLE.4) |> 
  filter(if_any(everything(), ~ . != "")) |> 
  count(CONTR_V4, name='FREQUENCY')

contr_v5 <- data |> 
  dplyr::select(CONTRIBUTING.FACTOR.VEHICLE.5) |> 
  mutate(CONTR_V5 = CONTRIBUTING.FACTOR.VEHICLE.5) |> 
  filter(if_any(everything(), ~ . != "")) |> 
  count(CONTR_V5, name='FREQUENCY')
```

```{r}
contr_factors_cleaned <- data |> 
  dplyr::select(starts_with("CONTRIBUTING.FACTOR.VEHICLE")) |>  # Selecciona todas las columnas relevantes
  pivot_longer(cols = everything(), names_to = "VEHICLE", values_to = "FACTOR") |>  # Convierte a formato largo
  filter(FACTOR != "") |>  # Filtra valores vacíos  
  mutate(FACTOR = recode(FACTOR,
    "Cell Phone (hand-Held)" = "Cell Phone (hand-held)",
    "Drugs (Illegal)" = "Drugs (illegal)",
    "Illnes" = "Illness",
    "Other Electronic Device" = "Other Devices",
    "Drugs (illegal)" = "Drugs (illegal)",
    "Unspecified" = "Unspecified",
    "Pedestrian/Bicyclist/Other Pedestrian Error/Confusion" = "Pedestrian Error/Confusion",
    "Aggressive Driving/Road Rage" = "Aggressive Driving"
  )) |> 
  count(FACTOR, name = "FREQUENCY") |>  # Cuenta ocurrencias de cada factor
  arrange(desc(FREQUENCY))  # Ordena de mayor a menor

```

```{r}
contr_factors_cleaned |> 
  arrange(FREQUENCY)
```

```{r}
contr_factors_cleaned |> 
  ggplot(aes(x = reorder(FACTOR, FREQUENCY), y = FREQUENCY)) + 
  geom_bar(stat = "identity", fill = "steelblue") + 
  # coord_flip() +  # Rota el gráfico para mejor visualización
  labs(title = "Total Frecuencia de Factores Contribuyentes",
       x = "Factor Contribuyente",
       y = "Frecuencia") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle=60, hjust=1,size=6),
        axis.title = element_text(size=10),
        title = element_text(size=12),
        axis.title.x=element_blank(),
        legend.title = element_blank()
        )
```

### Vamos a no considerar el factor 'Unspecified'

```{r}
contr_factors_cleaned |> 
  filter(FACTOR != 'Unspecified', FREQUENCY > 50000) |> 
  ggplot(aes(x = reorder(FACTOR, FREQUENCY), y = FREQUENCY, fill = FREQUENCY)) + 
  geom_bar(stat = "identity") + 
  # coord_flip() +  # Rota el gráfico para mejor visualización
  scale_fill_viridis_c(option = "plasma", name = "Frecuencia") +  # Usa la paleta 'plasma' para mejor contraste
  labs(title = "Frecuencia de Factores Contribuyentes en Accidentes",
       x = "Factor Contribuyente",
       y = "Frecuencia") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle=60, hjust=1,size=6),
        axis.text.y = element_text(size=7),
        axis.title = element_text(size=12),
        title = element_text(size=14, face="bold"),
        legend.position = "right")  # Mueve la leyenda a la derecha

contr_factors_cleaned |> 
  filter(FACTOR != 'Unspecified', FREQUENCY < 50000) |> 
  ggplot(aes(x = reorder(FACTOR, FREQUENCY), y = FREQUENCY, fill = FREQUENCY)) + 
  geom_bar(stat = "identity") + 
  # coord_flip() +  # Rota el gráfico para mejor visualización
  scale_fill_viridis_c(option = "plasma", name = "Frecuencia") +  # Usa la paleta 'plasma' para mejor contraste
  labs(title = "Frecuencia de Factores Contribuyentes en Accidentes",
       x = "Factor Contribuyente",
       y = "Frecuencia") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle=60, hjust=1,size=6),
        axis.text.y = element_text(size=7),
        axis.title = element_text(size=12),
        title = element_text(size=14, face="bold"),
        legend.position = "right")  # Mueve la leyenda a la derecha

```

## Vehicle Type Code:

### Jon

```{r}
vehicle_type_cleaned <- data |> 
  select(starts_with("VEHICLE.TYPE.CODE.1")) |>  
  pivot_longer(cols = everything(), names_to = "TYPE", values_to = "FACTOR") |> 
  filter(FACTOR != "") |>  
  count(FACTOR, name = "FREQUENCY") |>
  arrange(desc(FREQUENCY))

```

Paso 1: Calcular la Similitud Entre Todos los Tipos de Vehículos

```{r}
library(stringdist)

# Extraer los nombres únicos de los vehículos
vehicle_types <- unique(vehicle_type_cleaned$FACTOR)

# Crear una matriz de distancias entre todos los nombres
dist_matrix <- stringdistmatrix(vehicle_types, vehicle_types, method = "jw") 

# Convertir la matriz en un formato adecuado para clustering
rownames(dist_matrix) <- vehicle_types
colnames(dist_matrix) <- vehicle_types
```

Paso 2: Aplicar Clustering para Encontrar Grupos Similares

```{r}
# Realizar clustering jerárquico
hc <- hclust(as.dist(dist_matrix), method = "ward.D2")

# Elegir un número de clusters basado en la distancia (puedes ajustar `h`)
clusters <- cutree(hc, h = 0.25)  # Ajusta h entre 0.1 y 0.3 según el resultado

# Crear un dataframe con la asignación de clusters
df_clusters <- data.frame(VEHICLE_TYPE = vehicle_types, CLUSTER = clusters)
```

Paso 3: Unificar los Tipos de Vehículos en una Nueva Columna

```{r}
# Seleccionar un nombre representativo para cada cluster
df_clusters <- df_clusters |> 
  group_by(CLUSTER) |> 
  mutate(STANDARD_NAME = first(VEHICLE_TYPE))  # Puedes cambiar la estrategia de selección

# Unir esta información con los datos originales
vehicle_type_cleaned <- vehicle_type_cleaned |> 
  left_join(df_clusters, by = c("FACTOR" = "VEHICLE_TYPE")) |> 
  mutate(FACTOR = STANDARD_NAME) |> 
  select(-CLUSTER, -STANDARD_NAME)

# Contar la frecuencia de cada tipo unificado
vehicle_type_final <- vehicle_type_cleaned |> 
  count(FACTOR, name = "FREQUENCY") |> 
  arrange(desc(FREQUENCY))
```

### Rubo

```{r}
dataVT <- data |> 
  dplyr::select(VEHICLE.TYPE.CODE.1, VEHICLE.TYPE.CODE.2, VEHICLE.TYPE.CODE.3, VEHICLE.TYPE.CODE.4, VEHICLE.TYPE.CODE.5)

missing_data <- dataVT %>%
  summarise(across(everything(), ~ sum(is.na(.) | (is.numeric(.) & is.nan(.)) | .=="")))

missing_data_long <- missing_data %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Missing_Count")


# Crear el gráfico de barras
ggplot(missing_data_long, aes(x = Variable, y = Missing_Count, fill = Variable)) +
  geom_col() +  # Gráfico de barras
  labs(
    x = "Variable",  # Etiqueta del eje X
    y = "Número de valores problemáticos",  # Etiqueta del eje Y
    title = "Valores faltantes, no numéricos o cadenas vacías por variable"  # Título
  ) +
  theme_minimal() +  # Tema minimalista
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotar etiquetas del eje X
```


