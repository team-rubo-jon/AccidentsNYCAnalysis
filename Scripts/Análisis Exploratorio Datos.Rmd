---
title: "Análisis Exploratorio Datos"
output: html_document
---

## Cargar librerías y lectura de datos:

```{r, message=FALSE}
library(tidyverse)
library(viridis)
library(leaflet)
library(ggplot2)
library(dplyr)
library(GGally)
library(factoextra)
library(cluster)
library(tsibble)
library(psych)
library(caret)
library(MASS)
library(ca)
```

```{r}
#data <- read.csv("Vehicle_Collisions.csv")
```

## Lectura datos (temporal)

Jon:

```{r}
data <- read.csv("/DatosJon/UPNA/3º Ciencia de Datos/S6/Analisis Multivariante y Visualizacion de Datos/Trabajo/Datos/Vehicle_Collisions.csv")
```

Rubén:

```{r}
#data <- read.csv("C:/Users/ruben/OneDrive/Escritorio/UPNA/CIENCIA DE DATOS/6ºSemestre/AnalisisMultivarianteVisualizacionDatos/Trabajo Final/Motor_Vehicle_Collisions_-_Crashes.csv")
```

Fermin:

```{r}
#data <- read.csv("C:/Users/mintx/Mi unidad/CdD UPNA/.3.2/Analisis multivariante y visualizacion de datos/Trabajo/Vehicle_Collisions.csv")
```

# INTRODUCCION

En nuestro conjunto de datos, se recoje información sobre accidentes de tráfico en la ciudad de Nueva York. La información proviene de la página de datos del gobierno de EEUU, específicamente del apartado correspondiente a la ciudad de Nueva York.

Cada individuo corresponde a un accidente de tráfico, que se representa como una fila en el conjunto de datos. Las columnas incluyen información como la fecha y hora del accidente, la ubicación, el tipo de vehículo, y los factores contribuyentes al accidente entre otros muchos. Los accidentes se han almacenado desde el 1/01/2013 hasta la actualidad.

Son accidentes en los que se ha rellenado un informe policial, por lo que podrían no incluirse algunos accidentes menores o accidentes que no han sido reportados a la policía. Ya que este informe es necesario para los accidentes donde hay fallecidos, o daños de al menos 1000\$.

Además, como la forma de recoger los datos ha ido cambiando a lo largo de los años, es posible que hace años no se almacenase información como puede ser las coordenadas, y que en los últimos años con la implementación de dispositivos electrónicos se almacenen las coordenadas exactas de cada accidente. También habrá diferencias en la información dependiendo de la forma en la que el agente de policia correspondiente haya guardado los datos, ya que no son sensores sin fallo los que determinan la mayoría de variables (como pueden ser el tipo de vehículo o factor contribuyentes de la colisión).

Con todo esto, en nuestro trabajo trataremos de analizar las opciones que nos da el conjunto, e ir mostrando los resultados obtenidos visualmente. Y así poder encontrar patrones, agrupar los datos, o ver si podemos encontrar relación entre las variables que nos permitan entender mejor el problema tratado.

# LIMPIEZA INICIAL Y MUESTREO

En primer lugar, observamos que el dataset contiene más de 2 millones de filas, pero que además muchas de las variables están prácticamente vacías o con información redundante. Además, dada la naturaleza de los datos geolocalizados, hay ejemplos que no contienen esa información y no nos son de ayuda.

Por tanto, vamos a realizar una primera limpieza del dataset, eliminando tanto variables redundantes o con poca información, como ejemplos que no nos aporten la información suficiente.

## Eliminar variables:

-   *ZIP.CODE* (ya tenemos la variable *BOROUGH* que contiene el barrio)
-   *ON.STREET.NAME*, *OFF.STREET.NAME*, *CROSS.STREET.NAME* (es información que ya se almacena en las coordenadas)
-   *VEHICLE.TYPE.CODE.3*, *VEHICLE.TYPE.CODE.4*, *VEHICLE.TYPE.CODE.5* (prácticamente todos son valores nulos)
-   *CONTRIBUTING.FACTOR.VEHICLE.2*, *CONTRIBUTING.FACTOR.VEHICLE.3*, *CONTRIBUTING.FACTOR.VEHICLE.4*, *CONTRIBUTING.FACTOR.VEHICLE.5* (prácticamente todos son valores nulos)
-   *COLLISION_ID* (es un identificador que no aporta información)
-   *LOCATION* (es la combinación exacta de las variables *LATITUDE* Y *LONGITUDE*)

```{r}
data_cleaned <- data |> 
  dplyr::select(-ZIP.CODE, -ON.STREET.NAME, -OFF.STREET.NAME, -CROSS.STREET.NAME, -VEHICLE.TYPE.CODE.3, -VEHICLE.TYPE.CODE.4, -VEHICLE.TYPE.CODE.5, -COLLISION_ID, -CONTRIBUTING.FACTOR.VEHICLE.5, -CONTRIBUTING.FACTOR.VEHICLE.4, -CONTRIBUTING.FACTOR.VEHICLE.3, -CONTRIBUTING.FACTOR.VEHICLE.2,-LOCATION)
```

Con el filtrado realizado, obtenemos un dataset mucho más manejable en cuanto a variables se refiere, que nos permitirá trabajar mejor el problema. Haciendo un pequeño resumen de las variables que nos quedan:

-   CRASH.DATE: Fecha en la que se produció el accidente (Date)
-   CRASH.TIME: Hora en la que se produció el accidente (Time)
-   BOROUGH: Barrio de Nueva York donde sucedió el accidente (String)
-   LATITUDE/LONGITUDE: Coordenadas geográficas (Int)
-   NUMBER.OF.(PERSONS/PEDESTRIANS/CYCLIST/MOTORIST).(KILLED/INJURIED): Número y tipo de personas implicadas en el accidente, diferenciando heridos y fallecidos (Int)
-   CONTRIBUTING.FACTOR.VEHICLE: Causa principal del accidente (String)
-   VEHICLE.TYPE.CODE.1: Tipo del primero vehiculo implicado (String)
-   VEHICLE.TYPE.CODE.2: Tipo del segundo vehiculo implicado (String)

## Eliminar individuos:

-   Individuos que no tengan *LATITUDE* y/o *LONGITUDE* (No nos sirven para localizar el accidente)
-   Individuos que no tengan ningún Vehiculo implicado en *VEHICLE.TYPE.CODE*
-   Individuos que no tengan barrio *BOROUGH* (ya que tenemos los suficientes que si tienen esa información)
-   Individuos que no tengan una razon que haya causado el accidente *CONTRIBUTING.FACTOR.VEHICLE*
-   Individuos con valor de localización erronea (Coordenadas 0) *LONGITUDE*

```{r}
data_cleaned <- data_cleaned |> 
  filter(
    !is.na(LATITUDE) & !is.na(LONGITUDE),
    !(is.na(VEHICLE.TYPE.CODE.1) & is.na(VEHICLE.TYPE.CODE.2)),
    !(VEHICLE.TYPE.CODE.1 == "" & VEHICLE.TYPE.CODE.2 == ""),
    !(BOROUGH == ""),
    !(CONTRIBUTING.FACTOR.VEHICLE.1 == "Unspecified"),
    !(CONTRIBUTING.FACTOR.VEHICLE.1 == ""),
    !(LATITUDE==0),
    !(LONGITUDE==0))
```

## Renombrar variables:

Antes de empezar a trabajar con los datos, vamos a renombrar ciertas variables para facilitar la programación.

```{r}
data_cleaned <- data_cleaned |> 
  rename(
    DATE = CRASH.DATE,
    TIME = CRASH.TIME,
    NUM_PERSONS_INJURED = NUMBER.OF.PERSONS.INJURED,
    NUM_PERSONS_KILLED = NUMBER.OF.PERSONS.KILLED,
    NUM_PEDESTRIANS_INJURED = NUMBER.OF.PEDESTRIANS.INJURED,
    NUM_PEDESTRIANS_KILLED = NUMBER.OF.PEDESTRIANS.KILLED,
    NUM_CYCLIST_INJURED = NUMBER.OF.CYCLIST.INJURED,
    NUM_CYCLIST_KILLED = NUMBER.OF.CYCLIST.KILLED,
    NUM_MOTORIST_INJURED = NUMBER.OF.MOTORIST.INJURED,
    NUM_MOTORIST_KILLED = NUMBER.OF.MOTORIST.KILLED,
    CAUSE = CONTRIBUTING.FACTOR.VEHICLE.1,
    VEHICLE_1 = VEHICLE.TYPE.CODE.1,
    VEHICLE_2 = VEHICLE.TYPE.CODE.2
  )
  
```

## Eliminación de filas incompletas:

### Personas afectadas:

Vamos a comprobar si el número de personas heridas o fallecidas coincide con el número de heridos o fallecidos que se han dado en la fila. Para ello, vamos a crear una tabla con los diferentes tipos de heridos y fallecidos, y luego vamos a comprobar si el número total coincide con el número de personas heridas o fallecidas que se han dado en la fila.

```{r}
type_injured <- data_cleaned |> 
  dplyr::select(NUM_PERSONS_INJURED, NUM_PEDESTRIANS_INJURED, NUM_CYCLIST_INJURED, NUM_MOTORIST_INJURED,
         NUM_PERSONS_KILLED, NUM_PEDESTRIANS_KILLED, NUM_CYCLIST_KILLED, NUM_MOTORIST_KILLED) |> 
  pivot_longer(cols = everything(), names_to = 'TYPE_OF_INJURED', values_to = 'NUMBER')

# quiero saber el total de de cada tipo

type_injured |> 
  group_by(TYPE_OF_INJURED) |> 
  summarise(NUMBER = sum(NUMBER)) |> 
  arrange(desc(NUMBER))

# ahora mostramos los indices de los que no coinciden
filas_erroneas <- data_cleaned |> 
  dplyr::select(NUM_PERSONS_INJURED, NUM_PEDESTRIANS_INJURED, NUM_CYCLIST_INJURED, NUM_MOTORIST_INJURED,
         NUM_PERSONS_KILLED, NUM_PEDESTRIANS_KILLED, NUM_CYCLIST_KILLED, NUM_MOTORIST_KILLED) |> 
  mutate(ROW_ID = row_number(),
         TOTAL_NUMBER_INJURED = rowSums(across(c(NUM_PEDESTRIANS_INJURED, NUM_CYCLIST_INJURED, NUM_MOTORIST_INJURED))),
         TOTAL_NUMBER_KILLED = rowSums(across(c(NUM_PEDESTRIANS_KILLED, NUM_CYCLIST_KILLED, NUM_MOTORIST_KILLED)))) |>
  dplyr::select(ROW_ID, NUM_PERSONS_INJURED, TOTAL_NUMBER_INJURED, NUM_PERSONS_KILLED, TOTAL_NUMBER_KILLED) |> 
  filter(NUM_PERSONS_INJURED != TOTAL_NUMBER_INJURED | NUM_PERSONS_KILLED != TOTAL_NUMBER_KILLED)

filas_erroneas
```

Podemos ver como existen filas que no cumplen, por ello eliminamos las filas erroneas, aquellas filas cuyo numero de personas heridas o fallecidas no coincide con el total de heridos o fallecidos que se han dado en la fila.

```{r}
indices_erroneos <- filas_erroneas |> 
  pull(ROW_ID)

data_cleaned <- data_cleaned[-indices_erroneos, ]

```

## Eliminacion de valores incorrectos de la variable 'CAUSE':

```{r}
data_cleaned <- data_cleaned |> 
  mutate(CAUSE = recode(CAUSE,
    "Cell Phone (hand-Held)" = "Cell Phone (hand-held)",
    "Drugs (Illegal)" = "Drugs (illegal)",
    "Illnes" = "Illness",
    "Other Electronic Device" = "Other Devices",
    "Drugs (illegal)" = "Drugs (illegal)",
    "Unspecified" = "Unspecified",
    "Pedestrian/Bicyclist/Other Pedestrian Error/Confusion" = "Pedestrian Error/Confusion",
    "Aggressive Driving/Road Rage" = "Aggressive Driving"
  )) |> 
  filter(CAUSE != 1, CAUSE != 80) 
```

## Formatear variable 'DATE':

```{r}
data_cleaned <- data_cleaned |> 
  mutate(DATE = as.Date(DATE, format = "%m/%d/%Y"))
```

## Estandarizar strings de las variables:

Para facilitar el trabajo con los datos y evitar valores duplicados, vamos a estandarizar las variables CAUSE, VEHICLE_1 y VEHICLE_2 convirtiéndolas completamente a mayúsculas.

```{r}
data_cleaned$VEHICLE_1 <- toupper(data_cleaned$VEHICLE_1)
data_cleaned$VEHICLE_2 <- toupper(data_cleaned$VEHICLE_2)
data_cleaned$CAUSE <- toupper(data_cleaned$CAUSE)
```

## Muestreo:

Una vez eliminados los individuos que no son de interes, el dataset se reduce a 893275 filas. Es más manejable pero seguimos teniendo demasiadas observaciones, por lo que para trabajar con los datos de forma más comoda hemos decidido hacer un muestreo de 20.000 observaciones

```{r}
set.seed(123)
data_sampled <- sample_n(data_cleaned, 20000)
```

Como a partir de ahora trabajaremos con esta muestra en lugar del dataset completo, por comodidad lo almacenaremos en un nuevo archivo para solo tener que leer este.

```{r}
write.csv(data_sampled, "data_sampled.csv", row.names = FALSE)
write.csv(data_sampled, "Scripts/App/data_sampled.csv", row.names = FALSE) # para la app
```

### Leer muestra:

```{r}
data_sampled <- read.csv("Scripts/data_sampled.csv")
```

*TEMPORAL, PARA NO ANDAR ABRIENDO EL ARCHIVO GRANDE*

```{r}
data_sampled$VEHICLE_1 <- toupper(data_sampled$VEHICLE_1)
data_sampled$VEHICLE_2 <- toupper(data_sampled$VEHICLE_2)
data_sampled$CAUSE <- toupper(data_sampled$CAUSE)
```
# ANALISIS DESCRIPTIVO

Una vez obtenido el dataset con el que trabajaremos, vamos a visualizar las diferentes variables y sus comportamientos para tratar de entender mejor con que datos estamos trabajando antes de aplicar las diferentes técnicas.

Vamos a mostrar un histograma por cada variable o variables que creamos que van conjuntamente, y así ver la distribución de los datos. Además, también mostraremos la matriz de correlación entre las variables numéricas, para ver si hay alguna relación entre ellas.

## Variables 'DATE' y 'TIME':

```{r}
data_sampled <- data_sampled |> 
  mutate(DATE = as.Date(DATE, format = "%m/%d/%Y"))

data_time <- data_sampled |> 
  mutate(DATE_MONTH = yearmonth(DATE),
         DATE_YEAR = year(DATE)) |> 
  add_count(DATE, name = "FREQ_DAY") |> 
  add_count(DATE_MONTH, name = "FREQ_MONTH") |> 
  add_count(DATE_YEAR, name = "FREQ_YEAR") |>
  dplyr::select(DATE, DATE_MONTH, DATE_YEAR, FREQ_DAY, FREQ_MONTH, FREQ_YEAR) |> 
  arrange(DATE)
  

data_time |>
  ggplot(aes(x = DATE, y = FREQ_DAY)) + 
  geom_line(color = "steelblue") + 
  labs(title = "Frecuencia de Accidentes por Fecha",
       x = "Fecha",
       y = "Frecuencia") +
  theme_minimal()

data_time |>
  ggplot(aes(x = DATE_MONTH, y = FREQ_MONTH)) + 
  geom_line(color = "steelblue") + 
  labs(title = "Frecuencia de Accidentes por Mes",
       x = "Mes",
       y = "Frecuencia") +
  theme_minimal()

data_time |>
  ggplot(aes(x = DATE_YEAR, y = FREQ_YEAR)) + 
  geom_line(color = "steelblue") + 
  labs(title = "Frecuencia de Accidentes por Año",
       x = "Año",
       y = "Frecuencia") +
  theme_minimal()

```

Mostramos también un gráfico de barras de la frecuencia de accidentes por día, mes y año. 

```{r}
data_time |> 
  group_by(DATE) |> 
  summarise(FREQ_DAY = sum(FREQ_DAY)) |> 
  ggplot(aes(x = DATE, y = FREQ_DAY)) + 
  geom_bar(stat = "identity", fill = "steelblue") + 
  labs(title = "Frecuencia de Accidentes por Dia",
       x = "Mes",
       y = "Frecuencia") +
  theme_minimal()

data_time |> 
  group_by(DATE_MONTH) |> 
  summarise(FREQ_MONTH = sum(FREQ_MONTH)) |> 
  mutate(MONTH = month(DATE_MONTH, label = TRUE)) |> 
  ggplot(aes(x = MONTH, y = FREQ_MONTH)) + 
  geom_bar(stat = "identity", fill = "steelblue") + 
  labs(title = "Frecuencia de Accidentes por Mes",
       x = "Mes",
       y = "Frecuencia") +
  theme_minimal()

# ahora por dia
data_time |> 
  ggplot(aes(x = DATE_YEAR, y = FREQ_YEAR)) + 
  geom_bar(stat = "identity", fill = "steelblue") + 
  labs(title = "Frecuencia de Accidentes por Año",
       x = "Año",
       y = "Frecuencia") +
  theme_minimal()
```

Se puede ver claramente como en el año del covid la frecuencia de accidentes disminuye significativamente, debido a que la gente no se movía tanto como en años anteriores. Sin embargo, la frecuencia de accidentes se mantiene casi constante, incluso llegando a disminuir en en 2022. Esto tambien se puede deber a que a raiz del covid la gente ha cambiado su forma de moverse, y ahora se mueven más en bicicleta o andando, lo que puede hacer que haya más accidentes de este tipo. También, hemos comprobado que después del covid, en EEUU, modificaron leyes de tráfico que hizo que la gente sea mas consciente de la seguridad vial, y por lo tanto, haya menos accidentes.

## Variable 'BOROUGH':


```{r, warning=FALSE}
# Mostramos el histograma de los barrios, además del box plot:
data_sampled |> 
  ggplot(aes(x = BOROUGH)) +
  geom_histogram(stat = "count", fill = "steelblue") +
  labs(title = "Distribución de Accidentes por Distrito",
       x = "Distrito",
       y = "Número de Accidentes")

```

*METER LA SERIE TEMPORAL Y AÑADIR ALGÚN COMENTARIO EN RELACIÓN A LO QUE VEMOS EN LAS GRÁFICAS*

## Variables 'LATITUDE' y 'LONGITUDE:

*METER LOS MAPAS DE MINTXO Y AÑADIR ALGÚN COMENTARIO EN RELACIÓN A LO QUE VEMOS EN LAS GRÁFICAS*

Para el caso de latitude y longitude, vamos a mostrar un mapa con la localización de los accidentes en el mapa de Nueva York. Para ello, vamos a usar la librería leaflet, que nos permite mostrar mapas interactivos.

```{r, results='asis', echo=FALSE}
data_sampled |> 
  leaflet() |> 
  addTiles() |> 
  addCircleMarkers(
    lng = ~LONGITUDE,
    lat = ~LATITUDE,
    radius = 2,
    color = "red",
    stroke = FALSE,
    fillOpacity = 0.6,
    popup = ~paste("Lat:", LATITUDE, "<br>Lon:", LONGITUDE)
  )
```

## Variables de Conteo de Personas Afectadas:

Ahora, podemos comprobar el número de peatones, ciclistas y motoristas que han resultado heridos o muertos en los accidentes, junto al total de heridos y muertos:

```{r, fig.width=15}
# 1. Crear un resumen total
datos_resumen <- data_sampled |> 
  summarise(
    Total_Heridos = sum(NUM_PERSONS_INJURED, na.rm = TRUE),
    Heridos_Pedestrians = sum(NUM_PEDESTRIANS_INJURED, na.rm = TRUE),
    Heridos_Cyclists = sum(NUM_CYCLIST_INJURED, na.rm = TRUE),
    Heridos_Motorists = sum(NUM_MOTORIST_INJURED, na.rm = TRUE),
    Total_Muertos = sum(NUM_PERSONS_KILLED, na.rm = TRUE),
    Muertos_Pedestrians = sum(NUM_PEDESTRIANS_KILLED, na.rm = TRUE),
    Muertos_Cyclists = sum(NUM_CYCLIST_KILLED, na.rm = TRUE),
    Muertos_Motorists = sum(NUM_MOTORIST_KILLED, na.rm = TRUE)
  )

# 2. Pasar a formato largo
datos_long <- datos_resumen |> 
  pivot_longer(cols = everything(),
               names_to = c("Estado", "Tipo"),
               names_sep = "_",
               values_to = "Total")

# 3. Gráfico separado por estado (herido/muerto)
datos_long |> 
ggplot(aes(x = Tipo, y = Total, fill = Tipo)) +
  geom_col(show.legend = TRUE) +  # Activamos la leyenda
  geom_text(aes(label = Total), vjust = -0.5, size = 4) +
  facet_wrap(~Estado, scales = "free_y") +
  labs(
    title = "Total de personas heridas y muertas por tipo",
    x = NULL,  # Quitamos la etiqueta del eje x
    y = "Total",
    fill = "Tipo de persona"  # Título de la leyenda
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),       # Oculta los nombres debajo de las barras
    axis.ticks.x = element_blank(),      # Oculta las marcas del eje x
    legend.position = "bottom"           # Posiciona la leyenda debajo del gráfico
  )
```

Veamos ahora la matriz de correlación:

```{r, fig.width=15, message=FALSE}
ggpairs(data_sampled, columns = c("NUM_PERSONS_INJURED", "NUM_PEDESTRIANS_INJURED", "NUM_CYCLIST_INJURED", "NUM_MOTORIST_INJURED", "NUM_PERSONS_KILLED", "NUM_PEDESTRIANS_KILLED", "NUM_CYCLIST_KILLED", "NUM_MOTORIST_KILLED"), 
        title = "Matriz de Correlación entre Variables")
```

*No se yo si la correlación tiene mucho sentigo hacerlo o si es significativa...*

Vamos a mostrar un histograma por cada variable o variables que creamos que van conjuntamente, y así ver la distribución de los datos. Además, también mostraremos la matriz de correlación entre las variables numéricas, para ver si hay alguna relación entre ellas.

## Variable 'CAUSE':

Primero, vamos a ver la frecuencia de las distintas causas de los accidentes registrados. Para ello, vamos a crear una tabla con los diferentes factores contribuyentes y su frecuencia.

```{r}
causes_cleaned <- data_sampled |> 
  count(CAUSE, name = "FREQUENCY") |>  
  arrange(desc(FREQUENCY))

```

Nos ayudamos de un gráfico de barras para mostrar las frecuencias.

```{r, fig.width=13}
causes_cleaned |> 
  ggplot(aes(x = reorder(CAUSE, FREQUENCY), y = FREQUENCY)) + 
  geom_bar(stat = "identity", fill = "steelblue") + 
  coord_flip() +  # Rota el gráfico para mejor visualización
  labs(title = "Frecuencia de Factores Contribuyentes de los Accidentes",
       x = "Causas del Accidente",
       y = "Frecuencia") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle=60, hjust=1,size=6),
        axis.title = element_text(size=10),
        title = element_text(size=12),
        axis.title.x=element_blank(),
        legend.title = element_blank()
        )
```

## Variable 'VEHICLE_1' y 'VEHICLE_2':

Primero, vamos a ver la frecuencia de las distintos vehículos involucrados en los accidentes registrados. Para ello, vamos a crear una tabla para cada variable con su frecuencia.

```{r}
# Frecuencia de la variable VEHICLE_1:
V1_cleaned <- data_sampled |> 
  count(VEHICLE_1, name = "FREQUENCY_V1") |>  
  arrange(desc(FREQUENCY_V1))

# Frecuencia de la variable VEHICLE_1:
V2_cleaned <- data_sampled |> 
  count(VEHICLE_2, name = "FREQUENCY_V2") |>  
  arrange(desc(FREQUENCY_V2))

```

Ahora vamos a graficar los tipos de vehículos y sus frecuencias para *VEHICLE_1* y *VEHICLE_2*. Para ello, vamos a usar un gráfico de barras. Comenzamos con la primera variable:

```{r, fig.width=13}
V1_cleaned |> 
  filter(FREQUENCY_V1 > 3) |> 
  ggplot(aes(x = reorder(VEHICLE_1, FREQUENCY_V1), y = FREQUENCY_V1)) + 
  geom_bar(stat = "identity", fill = "steelblue") + 
  coord_flip() +  # Rota el gráfico para mejor visualización
  labs(title = "Frecuencia de Tipos de Vehículos (Primer involucrado)",
       x = "Vehículos",
       y = "Frecuencia") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle=60, hjust=1,size=6),
        axis.title = element_text(size=10),
        title = element_text(size=12),
        axis.title.x=element_blank(),
        legend.title = element_blank()
        )
```

De manera similar, mostramos la segunda variable:

```{r, fig.width=13}
V2_cleaned |> 
  filter(FREQUENCY_V2 > 3, VEHICLE_2 != '') |> 
  ggplot(aes(x = reorder(VEHICLE_2, FREQUENCY_V2), y = FREQUENCY_V2)) + 
  geom_bar(stat = "identity", fill = "steelblue") + 
  coord_flip() +  # Rota el gráfico para mejor visualización
  labs(title = "Frecuencia de Tipos de Vehículos (Primer involucrado)",
       x = "Vehículos",
       y = "Frecuencia") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle=60, hjust=1,size=6),
        axis.title = element_text(size=10),
        title = element_text(size=12),
        axis.title.x=element_blank(),
        legend.title = element_blank()
        )
```

Dado que en ambas variables había vehículos con una frecuencia de aparición menor o igual a 3, consideramos que no era relevante incluirlos en el gráfico, ya que dificultaban su interpretación.

# METODOLOGÍA MULTIVARIANTE

## PCA

Vamos a realizar un análisis de componentes principales (PCA) para reducir la dimensionalidad de los datos y ver si podemos encontrar patrones en los datos. Sin embargo, lo primero que vamos hacer es comprobar si tiene sentido hacer un pca con los datos numéricos que tenemos. Aunque en la matriz de correlación ya hemos visto que no hay correlaciones significativas en los datos, vamos a comprobar mediante estadísticos si los datos son adecuados para realizar un PCA.

Realizar el test de KMO y la prueba de esfericidad de Bartlett para comprobar si los datos son adecuados para realizar un PCA:

1.  Test de KMO: El test de KMO mide la adecuación de los datos para realizar un PCA. Un valor de KMO mayor a 0.6 indica que los datos son adecuados para realizar un PCA.

```{r}
datos_numeric <- data_sampled |> 
  dplyr::select(NUM_PERSONS_INJURED, NUM_PEDESTRIANS_INJURED, NUM_CYCLIST_INJURED, NUM_MOTORIST_INJURED,
         NUM_PERSONS_KILLED, NUM_PEDESTRIANS_KILLED, NUM_CYCLIST_KILLED, NUM_MOTORIST_KILLED)

KMO(datos_numeric)

```

Observamos que todas las variables tienen un KMO menor a 0.6, por lo que no es interesante realizar un PCA.

2.  Test de esfericidad de Bartlett:

```{r}
cortest.bartlett(cor(datos_numeric), n = nrow(datos_numeric))
```

El test de esfericidad de Bartlett acepta (p-valor\<0.05) la hipótesis nula de que la matriz de correlación es una matriz identidad. Por lo tanto, los datos se encuentran correlados.

En conclusión, pese a que los datos, según el test de esfericidad de Bartlett se encuentran correlados, no es interesante realizar un PCA, ya que el KMO no es adecuado. Sin embargo, igual es interesante analizar la posibilidad de realizar PCA agrupando las variables por heridos y muertos.

### PCA con variables agrupadas:

#### PCA con heridos

```{r}
# Seleccionar solo las variables de personas heridas
datos_heridos <- data_sampled |> 
  dplyr::select(NUM_PERSONS_INJURED, NUM_PEDESTRIANS_INJURED, 
         NUM_CYCLIST_INJURED, NUM_MOTORIST_INJURED)

# Escalar (normalizar) los datos
datos_heridos_scaled <- scale(datos_heridos)

```

Igual que antes, analizamos la posibilidad de realizar un PCA con los datos agrupados. Para ello, vamos a realizar el test de KMO y la prueba de esfericidad de Bartlett.

```{r}
# Calcular KMO
KMO(datos_heridos_scaled)

# Test de Bartlett
cortest.bartlett(cor(datos_heridos_scaled), n = nrow(datos_heridos_scaled))

```

De manera similar al caso general, obtenemos que no es interesante realizar un PCA, pese a que los datos se encuentran relacionados (p-valor\<0.05 en el test de Bartlett) el KMO vuelve a rechazar la idea de realizar el PCA, obtenemos un valor muy por debajo de 0.6.

#### PCA con muertos

```{r}
# Seleccionar solo las variables de personas muertas
datos_muertos <- data_sampled |> 
  dplyr::select(NUM_PERSONS_KILLED, NUM_PEDESTRIANS_KILLED, 
         NUM_CYCLIST_KILLED, NUM_MOTORIST_KILLED)

# Escalar (normalizar) los datos
datos_muertos_scaled <- scale(datos_muertos)
```

Al igual que antes, analizamos la posibilidad de realizar un PCA con los datos agrupados. Para ello, vamos a realizar el test de KMO y la prueba de esfericidad de Bartlett.

```{r}
# Calcular KMO
KMO(datos_muertos_scaled)

# Test de Bartlett
cortest.bartlett(cor(datos_muertos_scaled), n = nrow(datos_muertos_scaled))
```

De nuevo, obtenemos la misma conclusión que en los dos casos anteriores. Por lo tanto, no es interesante realizar un PCA con los datos agrupados.


## Clusterizaciones

### 1. Seleccionar variables numéricas relevantes:

```{r}
data_cluster <- data_sampled |> 
  dplyr::select(NUM_PERSONS_INJURED, NUM_PEDESTRIANS_INJURED, NUM_CYCLIST_INJURED, NUM_MOTORIST_INJURED,
         NUM_PERSONS_KILLED, NUM_PEDESTRIANS_KILLED, NUM_CYCLIST_KILLED, NUM_MOTORIST_KILLED)

# Escalar (normalizar) los datos
data_scaled <- scale(data_cluster)
```

### 2. Determinar el número óptimo de clusters:

```{r}
wss <- vector()
set.seed(123)
for (k in 1:10) {
  km <- kmeans(data_scaled, centers = k, nstart = 5)  # menos repeticiones
  wss[k] <- km$tot.withinss
}

plot(1:10, wss, type = "b", pch = 19,
     xlab = "Número de clusters",
     ylab = "Suma de cuadrados intra-clúster (WSS)",
     main = "Método del codo (optimizado)")

```

### 3. Realizar el clustering y visualizar:

```{r}
set.seed(123)
kmeans_result <- kmeans(data_scaled, centers = 6, nstart = 25)

# Visualizar clústeres en el espacio PCA
fviz_cluster(kmeans_result, data = data_scaled,
             ellipse.type = "norm",
             geom = "point",
             palette = "jco",
             ggtheme = theme_minimal())

```

Vemos que no se obtiene nada claro. Vamos a probar con otro tipo de clustering, el clustering jerárquico.

### Clustering jerarquico

```{r}
# Calcular la matriz de distancias
dist_matrix <- dist(data_scaled, method = "euclidean")

# Realizar el clustering jerárquico
hc <- hclust(dist_matrix, method = "complete") # --> Mirar a ver que método es mejor
```


```{r}
# Visualizar el dendrograma
plot(hc, labels = FALSE, hang = -1, main = "Dendrograma de Clustering Jerárquico")

# Cortamos:
rect.hclust(hc, k = 3, border = "red")
clusters <- cutree(hc, k = 3)
table(clusters)
```

### Agrupar los datos por barrios y clusterizar:

```{r}
# Este se puede borrar ya que no tiene mucho sentido:
data_borough_sum <- data_sampled |> 
  group_by(BOROUGH) |> 
  summarise(
    total_injured = sum(NUM_PERSONS_INJURED, na.rm = TRUE),
    total_killed = sum(NUM_PERSONS_KILLED, na.rm = TRUE),
    total_ped_injured = sum(NUM_PEDESTRIANS_INJURED, na.rm = TRUE),
    total_cyclist_injured = sum(NUM_CYCLIST_INJURED, na.rm = TRUE),
    total_motorist_injured = sum(NUM_MOTORIST_INJURED, na.rm = TRUE),
    total_ped_killed = sum(NUM_PEDESTRIANS_KILLED, na.rm = TRUE),
    total_cyclist_killed = sum(NUM_CYCLIST_KILLED, na.rm = TRUE),
    total_motorist_killed = sum(NUM_MOTORIST_KILLED, na.rm = TRUE)
  ) |> 
  na.omit()

```

```{r}
# Sólo con el total de muertos y heridos:
data_borough_sum <- data_sampled |> 
  group_by(BOROUGH) |> 
  summarise(
    total_injured = sum(NUM_PERSONS_INJURED, na.rm = TRUE),
    total_killed = sum(NUM_PERSONS_KILLED, na.rm = TRUE)
  ) |> 
  na.omit()
```


Escalar los datos:

```{r}
data_scaled <- data_borough_sum |> 
  dplyr::select(-BOROUGH) |> 
  scale()
```

Clustering jerárquico:

```{r}
dist_matrix <- dist(data_scaled)
hc <- hclust(dist_matrix, method = "complete")

# Dendrograma
plot(hc, labels = data_borough_sum$BOROUGH, main = "Dendrograma de barrios (suma total)")
rect.hclust(hc, k = 3, border = "blue")  # Prueba con k = 2, 3, 4, etc.
```

Añadir un gráfico de barras comparando las variables entre los clústeres.

El análisis de clúster jerárquico aplicado a los distritos de Nueva York ha dado como resultado una agrupación en tres clústeres bien diferenciados: por un lado, se agrupan el Bronx y Manhattan; por otro, Brooklyn y Queens; y finalmente, Staten Island queda separado en un grupo propio. Esta clasificación cobra sentido cuando se analiza en función de la densidad de población de cada distrito.

Manhattan, con 27.330 habitantes por kilómetro cuadrado, y el Bronx, con 12.831, presentan las mayores densidades de población entre los distritos analizados. Esta alta concentración de personas suele asociarse a una mayor exposición al tráfico, lo que incrementa la probabilidad de accidentes y, por tanto, de víctimas. Esto justificaría que ambos formen parte del mismo grupo, ya que comparten patrones similares tanto en cuanto a intensidad del tráfico como al número total de heridos y fallecidos.

En un segundo grupo aparecen Brooklyn y Queens, con densidades de población intermedias (14.400 y 8.104,7 habitantes por km² respectivamente). Aunque Brooklyn supera al Bronx en densidad, su comportamiento en términos de accidentes se asemeja más al de Queens, probablemente por la estructura urbana de ambos: grandes áreas residenciales, presencia de autopistas y una combinación de zonas densas y otras más dispersas. Esta mezcla da lugar a un número total de víctimas por accidentes más moderado, lo que explica su asociación en un mismo clúster.

Por último, Staten Island se mantiene como un grupo aislado, con una densidad de apenas 1.782,2 habitantes por km². Este distrito, claramente menos poblado y urbanizado, muestra un patrón de accidentes significativamente distinto, con un número mucho menor de víctimas en comparación con el resto. Su baja densidad y características geográficas más suburbanas justifican su separación en un clúster independiente.

Esta clasificación por clústeres, por tanto, se alinea coherentemente con la densidad de población de cada distrito y permite explicar diferencias en el comportamiento de los accidentes de tráfico a lo largo de la ciudad.

Vamos a añadir un gráfico de barras comparando las variables entre los clústeres:

1. Asignamos los clúster al dataframe:

```{r}
# Cortar el dendrograma para obtener 3 clústeres
clusters <- cutree(hc, k = 3)

# Añadir la columna de clúster al dataframe original
data_borough_sum$cluster <- as.factor(clusters)

```

2. Convertimos a formato largo para ggplot:

```{r}
data_borough_long <- data_borough_sum |> 
  pivot_longer(cols = c(total_injured, total_killed), 
               names_to = "Variable", 
               values_to = "Total")
```

3. Creamos el gráfico de barras:

```{r}
ggplot(data_borough_long, aes(x = Variable, y = Total, fill = cluster)) +
  geom_col(position = "dodge") +
  facet_wrap(~BOROUGH) +
  labs(title = "Comparación de heridos y muertos por clúster y barrio",
       x = "Variable", y = "Total", fill = "Clúster") +
  theme_minimal()

```

Este gráfico demuestra que nuestra hipótesis anterior tiene sentido, ya que el clúster es capaz de agrupar los distintos distritos de una manera muy correcta en función del número total de heridos y muertos.

## Análisis discriminante

Vamos a predecir el distrito al que pertenece un accidente. Para ello vamos a aseguir los siguientes pasos:

### 1. Seleccionar las variables predictoras y agrupar categorías poco frecuentes:

```{r}
# Agrupar las 10 categorías más frecuentes de VEHICLE_1
top_veh_types <- data_sampled |> 
  count(VEHICLE_1, sort = TRUE) |> 
  slice_head(n = 10) |> 
  pull(VEHICLE_1)

data_sampled <- data_sampled |> 
  mutate(VEHICLE_1 = ifelse(VEHICLE_1 %in% top_veh_types, VEHICLE_1, "OTROS"))

# Agrupar las 10 categorías más frecuentes de VEHICLE_2
top_veh_types <- data_sampled |> 
  count(VEHICLE_2, sort = TRUE) |> 
  slice_head(n = 10) |> 
  pull(VEHICLE_2)

data_sampled <- data_sampled |> 
  mutate(VEHICLE_2 = ifelse(VEHICLE_2 %in% top_veh_types, VEHICLE_2, "OTROS"))

# Agrupar las 10 categorías más frecuentes de CAUSE
top_causes <- data_sampled |> 
  count(CAUSE, sort = TRUE) |> 
  slice_head(n = 10) |> 
  pull(CAUSE)

data_sampled <- data_sampled |> 
  mutate(CAUSE = ifelse(CAUSE %in% top_causes, CAUSE, "OTROS"))
```


Creamos una columna de hora numérica: *VOY A PROBAR SIN AÑADIR HOUR, SINO INCLUYENDO TIME*

```{r}
data_sampled <- data_sampled |> 
  mutate(HOUR = as.numeric(substr(TIME, 1, 2)))
```

Seleccionamos las variables: *VOY HACER SIN TIME, LATITUDE y LONGITUDE*

```{r}
data_lda <- data_sampled |> 
  dplyr::select(BOROUGH,
         NUM_PERSONS_INJURED, NUM_PERSONS_KILLED,
         NUM_PEDESTRIANS_INJURED, NUM_PEDESTRIANS_KILLED,
         NUM_CYCLIST_INJURED, NUM_CYCLIST_KILLED,
         NUM_MOTORIST_INJURED, NUM_MOTORIST_KILLED,
         VEHICLE_1, VEHICLE_2, CAUSE) |>
  na.omit()
```


### 2. Convertir las variables categóricas en dummies:

```{r}
data_lda_dummy <- dummyVars(BOROUGH ~ ., data = data_lda)
data_lda_transformed <- predict(data_lda_dummy, newdata = data_lda)
data_lda_final <- data.frame(BOROUGH = data_lda$BOROUGH, data_lda_transformed)
```

### 3. Crear un conjunto de entrenamiento y test:

```{r}
set.seed(123)
train_index <- createDataPartition(data_lda_final$BOROUGH, p = 0.7, list = FALSE)
train_data <- data_lda_final[train_index, ]
test_data <- data_lda_final[-train_index, ]
```

### 4. Aplicamos el análisis discriminante:

```{r}
# Aplicar LDA
lda_model <- lda(BOROUGH ~ ., data = train_data)
lda_model

```

### 5. Evaluamos el modelo:

```{r}
predictions <- predict(lda_model, newdata = test_data)

# Matriz de confusión
table(Predicho = predictions$class, Real = test_data$BOROUGH)
```

### 6. Visualizar las dos primeras funciones discriminantes:

```{r}
lda_df <- data.frame(predictions$x, BOROUGH = test_data$BOROUGH)

ggplot(lda_df, aes(x = LD1, y = LD2, color = BOROUGH)) +
  geom_point(alpha = 0.5) +
  labs(title = "Proyección LDA: Accidentes por distrito",
       x = "Función discriminante 1",
       y = "Función discriminante 2") +
  theme_minimal()
```

Vemos que no tiene sentido hacer este análisis, sólo reliza buenas predicciones si disponemos de las variables LONGITUDE y LATITUDE. Pero estamos incurriendo a una especie de "trampa".

Sin embargo, sí que puede ser interesante predecir la causa del accidente en función de distintas variables. Teniendo en cuenta que vamos a coger las 10 más frecuentes y el resto, se incluirá como causa OTROS:

```{r}
# 1. Crear variable VEHICLE_COUNT --> Si hay un segundo vehículo incluimos 2, sino 1.
data_sampled <- data_sampled |> 
  mutate(VEHICLE_COUNT = ifelse(is.na(VEHICLE_2), 1, 2)) |> 
  mutate(CAUSE = as.factor(CAUSE))

# 2. Quedarse con las 5 causas más frecuentes
top_causes <- data_sampled |> 
  count(VEHICLE_1, sort = TRUE) |> 
  slice_head(n = 10) |> 
  pull(VEHICLE_1)

data_lda <- data_sampled |> 
  filter(VEHICLE_1 %in% top_causes) |> 
  mutate(CAUSE = factor(VEHICLE_1),
         BOROUGH = factor(BOROUGH),
         HOUR = as.numeric(format(strptime(TIME, format = "%H:%M"), "%H"))) |> 
  dplyr::select(CAUSE, HOUR, BOROUGH, VEHICLE_COUNT, 
         NUM_PERSONS_INJURED, NUM_PERSONS_KILLED) |> 
  na.omit()

# 3. Codificar variables categóricas (BOROUGH) como dummies
data_dummies <- dummyVars(CAUSE ~ ., data = data_lda)
data_transformed <- predict(data_dummies, newdata = data_lda)
data_final <- data.frame(CAUSE = data_lda$CAUSE, data_transformed)

# 4. Dividir en entrenamiento y prueba
set.seed(123)
train_index <- createDataPartition(data_final$CAUSE, p = 0.7, list = FALSE)
train_data <- data_final[train_index, ]
test_data <- data_final[-train_index, ]

train_data <- train_data |> dplyr::select(-NUM_PERSONS_KILLED, -VEHICLE_COUNT)
test_data  <- test_data  |> dplyr::select(-NUM_PERSONS_KILLED, -VEHICLE_COUNT)

# 5. Modelo LDA
lda_model <- lda(CAUSE ~ ., data = train_data)

# 6. Predicción y matriz de confusión
predictions <- predict(lda_model, newdata = test_data)
conf_matrix <- table(Predicho = predictions$class, Real = test_data$CAUSE)
print(conf_matrix)

```

## Random forest

```{r}
# Paquetes necesarios
library(randomForest)

# 1. Agrupar las poco frecuentes:

# Función para agrupar niveles poco frecuentes
group_rare_levels <- function(x, threshold = 0.01) {
  freq <- prop.table(table(x))
  rare_levels <- names(freq[freq < threshold])
  x <- as.character(x)
  x[x %in% rare_levels] <- "OTROS"
  factor(x)
}

# Aplicar a los datos
data_sampled <- data_sampled |> 
  mutate(
    VEHICLE_1 = group_rare_levels(VEHICLE_1),
    VEHICLE_2 = group_rare_levels(VEHICLE_2),
    CAUSE = group_rare_levels(CAUSE)
  )

data_sampled <- data_sampled |> 
  mutate(HOUR = as.numeric(format(strptime(TIME, format = "%H:%M"), "%H")))

# 2. Crear conjunto de entrenamiento y test
set.seed(42)
n <- nrow(data_sampled)
train_index <- sample(1:n, size = floor(0.7 * n))
train_data <- data_sampled[train_index, ] |> 
  dplyr::select(-TIME, -DATE)
test_data <- data_sampled[-train_index, ] |> 
  dplyr::select(-TIME, -DATE)

# 3. Convertir variables categóricas a factor
train_data <- train_data |> mutate(across(where(is.character), as.factor))
test_data <- test_data |> mutate(across(where(is.character), as.factor))

# 4. Ajustar modelo Random Forest
rf_model <- randomForest(CAUSE ~ ., data = train_data, ntree = 100, importance = TRUE)

# 5. Predicciones y matriz de confusión
predictions <- predict(rf_model, newdata = test_data)
conf_matrix <- table(Predicho = predictions, Real = test_data$CAUSE)
as.table(conf_matrix)

# 6. Importancia de variables
importance(rf_model)
varImpPlot(rf_model)



```

### Gráficas de resultados:

#### Matriz de confusión:

```{r}
# Convertir la matriz de confusión en un formato más adecuado para ggplot
conf_matrix_df <- as.data.frame(as.table(conf_matrix))

# Graficar la matriz de confusión
ggplot(conf_matrix_df, aes(x = Predicho, y = Real, fill = Freq)) + 
  geom_tile() + 
  geom_text(aes(label = Freq), color = "white", size = 4) + 
  scale_fill_gradient(low = "white", high = "blue") + 
  theme_minimal() +
  labs(title = "Matriz de Confusión", x = "Predicho", y = "Real")
```

#### Curva ROC:

```{r}
library(pROC)

# Calcular la curva ROC
roc_curve <- roc(test_data$CAUSE, as.numeric(predictions))
plot(roc_curve, main = "Curva ROC")
```

#### Importancia de las variables:

```{r}
# Visualizar la importancia de las variables con un gráfico
var_imp <- importance(rf_model)
var_imp_df <- data.frame(Variable = rownames(var_imp), Importance = var_imp[, 1])

# Graficar
ggplot(var_imp_df, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Importancia de las Variables", x = "Variable", y = "Importancia")

```

#### Distribución de las predicciones

```{r}
# Gráfico de distribución de las predicciones y las clases reales
ggplot(data.frame(Predicho = predictions, Real = test_data$CAUSE), aes(x = Predicho, fill = Real)) +
  geom_bar(position = "dodge") +
  labs(title = "Distribución de Predicciones vs Clases Reales", x = "Predicción", y = "Frecuencia")

```

#### Gráfico de dependencia parcial:

```{r}
# Gráfico de dependencia parcial
library(pdp)
partial_plot <- partial(rf_model, pred.var = "HOUR", grid.resolution = 50)
plot(partial_plot)

```

 
## Análisis de correspondencia

### 1. Tipo de vehiculo y causa:

```{r, fig.width=15}
# Crear una tabla de contingencia
contingency_table <- table(data_sampled$VEHICLE_1, data_sampled$CAUSE)

# Realizar el análisis de correspondencia
ca_result <- ca(contingency_table)

# Resumen del análisis
summary(ca_result)

# Visualizar el gráfico de correspondencia
plot(ca_result)
```

### 2. Tipo de vehiculo y hora:

```{r, fig.width=15}
# Crear una tabla de contingencia
contingency_table <- table(data_sampled$VEHICLE_1, data_sampled$HOUR)

# Realizar el análisis de correspondencia
ca_result <- ca(contingency_table)

# Resumen del análisis
summary(ca_result)

# Visualizar el gráfico de correspondencia
plot(ca_result)
```


### 3. Causa del accidente y hora:

```{r}
# Crear una tabla de contingencia
contingency_table <- table(data_sampled$CAUSE, data_sampled$HOUR)

# Realizar el análisis de correspondencia
ca_result <- ca(contingency_table)

# Resumen del análisis
summary(ca_result)

# Visualizar el gráfico de correspondencia
plot(ca_result)
```

### 4. Causa del accidente y día de la semana: 

```{r}
# Extraer el día de la semana de la variable TIME
data_sampled$DAY_OF_WEEK <- weekdays(as.Date(data_sampled$DATE, format = "%d/%m/%Y"))

# Crear una tabla de contingencia
contingency_table <- table(data_sampled$CAUSE, data_sampled$DAY_OF_WEEK)

# Realizar el análisis de correspondencia
ca_result <- ca(contingency_table)

# Resumen del análisis
summary(ca_result)

# Visualizar el gráfico de correspondencia
plot(ca_result)
```













### Variable 'CAUSE'

#### Prueba 1

Vamos a realizar una clusterización de los diferentes factores contribuyentes al accidente. Para ello, vamos a usar el algoritmo K-means, que nos permite agrupar los diferentes factores en diferentes grupos.

```{r}
cause_norm <- causes_cleaned |>
  mutate(FREQUENCY_NORM = scale(FREQUENCY)) |>
  dplyr::select(CAUSE, FREQUENCY_NORM)

rownames(cause_norm) <- cause_norm$CAUSE
cause_norm <- cause_norm |> 
  dplyr::select(-CAUSE)

fviz_nbclust(x = cause_norm, 
             FUNcluster = kmeans, 
             method = "wss", 
             k.max = 15, 
             nstart = 50) +
  labs(title = "Número óptimo de clusters (WSS)")

```

```{r, warning=FALSE, message=FALSE}

pam_clusters <- pam(x = cause_norm, k = 4, metric = "manhattan")

# Visualizar con fviz_cluster usando dos dimensiones
# fviz_cluster(pam_clusters, 
#              data = cause_norm,
#              show.clust.cent = TRUE, 
#              ellipse.type = "euclid",
#              star.plot = TRUE, 
#              repel = TRUE) +
#   labs(title = "Resultados clustering K-means (CAUSE)") +
#   theme_bw() + 
#   theme(legend.position = "none")


resultados <- data.frame(ciudad = names(pam_clusters$cluster), cluster = as.factor(pam_clusters$cluster)) |> 
    arrange(cluster)

library(igraph)
library(tidygraph)
library(ggraph)

datos_graph <- graph_from_data_frame(d = resultados, directed = TRUE)
datos_graph <- as_tbl_graph(datos_graph)

# Se añade información sobre a que cluster pertenece cada observacion
datos_graph <- datos_graph |> 
    activate(nodes) |> 
    left_join(resultados, by = c(name = "ciudad"))

ggraph(graph = datos_graph) + 
  geom_edge_link(alpha = 0.5) + 
  geom_node_point(aes(color = cluster)) +
  geom_node_text(aes(label = name), repel = TRUE, alpha = 0.5, size = 3) + 
  labs(title = "Resultados clustering K-means") +
  theme_graph()

```

#### Prueba 2

Rpealizar PCA con otro tio de metricas.

1.  Crear una matriz de características para cada causa

```{r}

# Ejemplo: sumar heridos y muertos promedio por causa
cause_features <- data_sampled |> 
  group_by(CAUSE) |> 
  summarise(
    N = n(),
    avg_injured = mean(NUM_PERSONS_INJURED, na.rm = TRUE),
    avg_killed = mean(NUM_PERSONS_KILLED, na.rm = TRUE),
    # avg_cyclist_injured = mean(NUM_CYCLIST_INJURED, na.rm = TRUE),
    # avg_motorist_injured = mean(NUM_MOTORIST_INJURED, na.rm = TRUE),
    # avg_pedestrian_injured = mean(NUM_PEDESTRIANS_INJURED, na.rm = TRUE)
  ) |> 
  ungroup()

```

2.  Normalizar los datos y aplicar PCA

```{r}
# Normalizar
cause_features_scaled <- cause_features |> 
  column_to_rownames("CAUSE") |> 
  scale()

# PCA
pca_res <- prcomp(cause_features_scaled, center = TRUE, scale. = TRUE)
biplot(pca_res, main = "PCA de las causas")

library(ggbiplot)
ggbiplot(pca_res, choices = c(1,2))
```

3.  Clustering

```{r}
# Determinar número de clusters óptimo
fviz_nbclust(cause_features_scaled, kmeans, method = "wss")

# K-means clustering
set.seed(123)
km <- kmeans(cause_features_scaled, centers = 7, nstart = 50)

# Visualizar clusters
fviz_cluster(km, data = cause_features_scaled, 
             show.clust.cent = TRUE, 
             repel = TRUE) +
  theme_minimal() +
  labs(title = "Clustering de causas de accidente")
```

#### Prueba 3

Establecer similitudes entre los nombres de las causas.

```{r}
library(stringdist)

# Crear matriz de distancias entre nombres de causas
dist_matrix <- stringdistmatrix(causes_cleaned$CAUSE, causes_cleaned$CAUSE, method = "jw")  # método Jaro-Winkler, o "lv" para Levenshtein
rownames(dist_matrix) <- causes_cleaned$CAUSE
colnames(dist_matrix) <- causes_cleaned$CAUSE
```

Clustering jerarquico

```{r, warning=FALSE}
hc <- hclust(as.dist(dist_matrix), method = "ward.D2")
plot(hc, main = "Dendograma de Causas", cex = 0.7)


fviz_dend(x = hc, cex = 0.5, main = "Dendrograma - ward", xlab = "observaciones",
    ylab = "distancia", sub = "", horiz = TRUE)

```

Cortar endograma

```{r, warning=FALSE}
fviz_dend(x = hc,
          k = 8,
          k_colors = c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd", "#8c564b",  "#e377c2", "#7f7f7f"),
          color_labels_by_k = TRUE,
          rect = TRUE,
          #rect_border = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
          rect_fill = TRUE,
          cex = 0.5,
          main = "Dendrograma - ward",
          xlab = "observaciones",
          ylab = "distancia",
          sub = "")

fviz_dend(x = hc, k = 8, k_colors = c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd", "#8c564b",  "#e377c2", "#7f7f7f"),
    color_labels_by_k = TRUE, cex = 0.5, type = "circular")

```

Ahora vamos a agrupar las causas en los 8 grupos

```{r}
# Asignar cada causa a uno de los 8 clusters
grupo_causas <- cutree(hc, k = 8)

# Crear data frame con causa original y grupo asignado
causas_grupo <- data.frame(
  CAUSE = names(grupo_causas),
  GRUPO_CAUSA = grupo_causas
)

# unimos al dataframe original
data_sampled <- data_sampled |> 
  left_join(causas_grupo, by = "CAUSE")


```

## Analisis discriminante

```{r}
library(MASS)

data_sampled$BOROUGH_NUM <- as.numeric(factor(data_sampled$BOROUGH))

modelo_lda <- lda(formula = GRUPO_CAUSA ~ NUM_PERSONS_INJURED + NUM_PERSONS_KILLED + BOROUGH_NUM, data = data_sampled)

new_data <- data_sampled[, c("NUM_PERSONS_INJURED", "NUM_PERSONS_KILLED", "BOROUGH_NUM")]

predicciones <- predict(object = modelo_lda, 
                        newdata = data_sampled[, c("NUM_PERSONS_INJURED", "NUM_PERSONS_KILLED", "NUM_PEDESTRIANS_INJURED", "NUM_PEDESTRIANS_KILLED", "NUM_CYCLIST_INJURED", "NUM_CYCLIST_KILLED", "NUM_MOTORIST_INJURED", "NUM_MOTORIST_KILLED", "BOROUGH_NUM")], 
                        method = "predictive")

table(data_sampled$GRUPO_CAUSA, predicciones$class, dnn = c("Clase real", "Clase predicha"))

```

Igual no tiene mucho sentido, predice casi siempre el 1.

# CONCLUSIONES
